{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ef5285e-dd7b-4802-a4f8-3d294bfe9f52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q1. What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eb57ff-3cf2-4fe7-80bf-fabbbbf409d6",
   "metadata": {},
   "source": [
    "A linear Support Vector Machine (SVM) aims to find a hyperplane that best separates classes in a feature space. The mathematical formula for a linear SVM can be expressed as follows:\n",
    "\n",
    "Given a set of training data points \\((x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\), where \\(x_i\\) is a feature vector and \\(y_i\\) is the class label (\\(y_i = \\pm 1\\) for binary classification), a linear SVM tries to find a hyperplane represented by:\n",
    "\n",
    "\\[w \\cdot x + b = 0\\]\n",
    "\n",
    "Where:\n",
    "- \\(w\\) is the weight vector, which is perpendicular to the hyperplane.\n",
    "- \\(x\\) is a feature vector.\n",
    "- \\(b\\) is the bias term (also known as the intercept).\n",
    "\n",
    "The decision function for a linear SVM is:\n",
    "\n",
    "\\[f(x) = w \\cdot x + b\\]\n",
    "\n",
    "The SVM classifies a data point \\(x\\) based on the sign of \\(f(x)\\). If \\(f(x) > 0\\), it's classified as one class, and if \\(f(x) < 0\\), it's classified as the other class.\n",
    "\n",
    "The objective of a linear SVM is to find the optimal \\(w\\) and \\(b\\) such that the margin (distance between the hyperplane and the nearest data points, also known as support vectors) is maximized while minimizing the classification error.\n",
    "\n",
    "Mathematically, this is formulated as a constrained optimization problem:\n",
    "\n",
    "\\[\n",
    "\\text{Minimize } \\frac{1}{2} ||w||^2\n",
    "\\]\n",
    "\n",
    "Subject to the constraints:\n",
    "\n",
    "\\[\n",
    "y_i(w \\cdot x_i + b) \\geq 1 \\text{ for } i = 1, 2, ..., n\n",
    "\\]\n",
    "\n",
    "These constraints ensure that all data points are correctly classified and that the margin is maximized.\n",
    "\n",
    "The Lagrangian of this optimization problem is then solved to find the optimal values of \\(w\\) and \\(b\\) using techniques like the Sequential Minimal Optimization (SMO) algorithm.\n",
    "\n",
    "This formulation of a linear SVM is for binary classification. It can be extended to multiclass classification using techniques like one-vs-rest or one-vs-one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d2db46-6736-4fe5-b0ab-1e5396da0e18",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af501a1-f7a1-4510-8a09-9561ce458f73",
   "metadata": {},
   "source": [
    "The objective function of a linear Support Vector Machine (SVM) is to find the parameters \\(w\\) and \\(b\\) that define a hyperplane with the maximum margin while minimizing the classification error. Mathematically, this can be formulated as an optimization problem.\n",
    "\n",
    "The objective function of a linear SVM can be written as:\n",
    "\n",
    "\\[ \\text{Minimize} \\quad \\frac{1}{2} ||w||^2 \\]\n",
    "\n",
    "subject to the constraints:\n",
    "\n",
    "\\[ y_i(w \\cdot x_i + b) \\geq 1 \\text{ for } i = 1, 2, ..., n \\]\n",
    "\n",
    "Here's what these terms represent:\n",
    "\n",
    "- \\(w\\) is the weight vector that defines the orientation of the hyperplane.\n",
    "- \\(b\\) is the bias term (or intercept) that shifts the hyperplane.\n",
    "- \\(x_i\\) represents the feature vectors.\n",
    "- \\(y_i\\) represents the class labels (\\(y_i = \\pm 1\\) for binary classification).\n",
    "- \\(n\\) is the number of training samples.\n",
    "\n",
    "The objective function \\(\\frac{1}{2} ||w||^2\\) is a regularization term. It encourages the model to find a hyperplane that maximizes the margin (i.e., the distance between the hyperplane and the nearest data points, which are called support vectors).\n",
    "\n",
    "The constraints \\(y_i(w \\cdot x_i + b) \\geq 1\\) ensure that all training samples are correctly classified and lie on the correct side of the margin. This is crucial for a well-generalized model.\n",
    "\n",
    "The goal of this optimization problem is to find the optimal \\(w\\) and \\(b\\) that satisfy these constraints while minimizing the regularization term. This is typically done using techniques like the Sequential Minimal Optimization (SMO) algorithm or quadratic programming solvers.\n",
    "\n",
    "By solving this optimization problem, the SVM finds the hyperplane that best separates the classes with the maximum margin, leading to a robust and accurate classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5104d37-7663-482c-936a-c5c93d8b04ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q3. What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52c99d7-027d-452a-b43b-47f9e4436d3b",
   "metadata": {},
   "source": [
    "The kernel trick is a fundamental concept in Support Vector Machines (SVMs) that allows them to efficiently handle non-linearly separable data by implicitly mapping it into a higher-dimensional feature space.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1. **Linear Separability in a Higher Dimension:**\n",
    "   - Sometimes, data that is not linearly separable in a lower-dimensional space can become linearly separable in a higher-dimensional space. For example, consider points arranged in a circle in a 2D plane. They cannot be separated by a straight line, but if you map them into a 3D space (using a transformation), you can separate them with a plane.\n",
    "\n",
    "2. **Avoiding Explicit Transformation:**\n",
    "   - Instead of explicitly transforming the data into a higher-dimensional space (which can be computationally expensive or even impractical for very high dimensions), the kernel trick allows SVMs to perform computations as if the transformation had been applied, without actually computing the transformed feature vectors.\n",
    "\n",
    "3. **Kernel Functions:**\n",
    "   - Kernels are mathematical functions that calculate the dot product (or a similarity measure) between two vectors in the higher-dimensional space. The key idea is that certain kernel functions implicitly represent transformations that map data into higher dimensions.\n",
    "\n",
    "   - Common kernel functions include:\n",
    "     - Linear Kernel: \\(K(x, x') = x \\cdot x'\\)\n",
    "     - Polynomial Kernel: \\(K(x, x') = (x \\cdot x' + c)^d\\)\n",
    "     - Radial Basis Function (RBF) Kernel (or Gaussian Kernel): \\(K(x, x') = \\exp\\left(-\\frac{||x - x'||^2}{2\\sigma^2}\\right)\\)\n",
    "\n",
    "4. **Efficient Computation:**\n",
    "   - Using the kernel trick, SVMs can compute dot products or similarities in the higher-dimensional space without actually having to explicitly transform the data. This is computationally more efficient.\n",
    "\n",
    "5. **Flexibility for Non-Linear Separation:**\n",
    "   - The kernel trick enables SVMs to find non-linear decision boundaries in the original input space.\n",
    "\n",
    "Overall, the kernel trick allows SVMs to handle complex, non-linearly separable data efficiently, making them a powerful tool for classification tasks in a wide range of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76269a4-48a9-4958-b009-cb458e456716",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q4. What is the role of support vectors in SVM Explain with example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a71585b-8a11-4833-9cb5-474a37883594",
   "metadata": {},
   "source": [
    "Support vectors play a crucial role in Support Vector Machines (SVMs). They are the data points that are closest to the decision boundary (hyperplane) and have the potential to influence the position and orientation of the hyperplane. The support vectors are the key elements in defining the margin, which is the distance between the hyperplane and the nearest data points.\n",
    "\n",
    "These are the data points closest to the decision boundary. The margin is defined by the distance between the hyperplane and these support vectors. It's important to note that the position and orientation of the hyperplane are determined by these support vectors.\n",
    "\n",
    "In a linear SVM, the other data points don't influence the position of the hyperplane as long as they are correctly classified and don't cross the margin boundaries. They are not part of the support vectors.\n",
    "\n",
    "If any of the support vectors were removed or moved, it would affect the position and orientation of the hyperplane. The SVM aims to maximize the margin while correctly classifying the data, and the support vectors are critical in achieving this.\n",
    "\n",
    "In summary, support vectors are the data points that are crucial in defining the hyperplane and the margin in an SVM. They play a central role in the algorithm's decision-making process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacc053f-4fae-4a0c-8cbe-e3250abf2eba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da72ef7-fb02-4253-82a8-c2e1461274a8",
   "metadata": {},
   "source": [
    "Certainly! I'll describe each concept and provide an example with accompanying graphs.\n",
    "\n",
    "### Hyperplane:\n",
    "\n",
    "A hyperplane is a decision boundary that separates two classes in a feature space. In a binary classification problem, it's a flat affine subspace of one dimension less than the input space.\n",
    "\n",
    "**Example:**\n",
    "Consider a 2D feature space with two classes, blue (class 0) and red (class 1). The hyperplane is represented by the equation \\(w \\cdot x + b = 0\\), which separates the classes.\n",
    "\n",
    "![Hyperplane Example](https://i.imgur.com/JbObxjw.png)\n",
    "\n",
    "### Marginal Plane:\n",
    "\n",
    "The marginal plane (also known as the \"margin\") is a region parallel to the hyperplane, bounded by the support vectors. It's the area where data points have an impact on the position and orientation of the hyperplane.\n",
    "\n",
    "**Example:**\n",
    "In the same 2D feature space, the marginal plane is the area between the dashed lines in the graph below:\n",
    "\n",
    "![Marginal Plane Example](https://i.imgur.com/3eHVoyU.png)\n",
    "\n",
    "### Soft Margin:\n",
    "\n",
    "In a soft-margin SVM, some misclassifications are allowed to achieve a more robust and generalizable model. The margin is \"softened,\" meaning that some points can fall within the margin or even on the wrong side of the hyperplane.\n",
    "\n",
    "**Example:**\n",
    "Consider a scenario where it's not possible to perfectly separate the classes. A soft-margin SVM allows for a certain number of misclassifications (represented by the points inside the margin).\n",
    "\n",
    "![Soft Margin Example](https://i.imgur.com/zC3Fc1H.png)\n",
    "\n",
    "### Hard Margin:\n",
    "\n",
    "In a hard-margin SVM, no misclassifications are allowed. The data must be perfectly separable by a hyperplane. This is a stricter condition compared to the soft-margin SVM.\n",
    "\n",
    "**Example:**\n",
    "In the case where classes are perfectly separable, a hard-margin SVM finds the optimal hyperplane with the maximum margin, ensuring all points are correctly classified.\n",
    "\n",
    "![Hard Margin Example](https://i.imgur.com/ZsbHfL2.png)\n",
    "\n",
    "Keep in mind that in real-world scenarios, it's common for data to not be perfectly separable. In such cases, soft-margin SVMs are more appropriate, as they provide a balance between maximizing the margin and allowing for some misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3e4940-ff42-4f52-bb87-24afbf10876a",
   "metadata": {},
   "source": [
    "# Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "070bacbe-74d8-43d2-9888-4d8be492ffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "datasets = load_iris()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "559b8157-e48e-42cd-9d26-e3be4ce2dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(datasets.data , columns=datasets.feature_names)\n",
    "\n",
    "df['target'] = datasets.target\n",
    "df.head()\n",
    "\n",
    "\n",
    "X = df.drop('target' , axis=1)\n",
    "y = df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e4585c6-6683-4d1c-b8d6-e18f2400923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                   test_size=0.20,\n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd960dc-ef9f-4c5e-9b35-3e0426af47a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "model.fit(X_train , y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_pred , y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "330d934e-4ccc-4589-a956-b8ccb6d32df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 2, 'kernel': 'poly'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "'kernel' : ['poly' , 'rbf' , 'sigmoid'],\n",
    "'C' : [1,2,3,4,5,6,7,8,9,10]\n",
    "}\n",
    "\n",
    "\n",
    "clf = GridSearchCV(model , param_grid=param_grid , cv=5)\n",
    "\n",
    "clf.fit(X_train , y_train)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b513f1-3c80-475a-be53-7222893095cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45ea496c-7fe8-4a42-a19b-11c00f172923",
   "metadata": {},
   "source": [
    "# Bonus task: Implement a linear SVM classifier from scratch using Python and compare its performance with the scikit-learn implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f65a01ec-cd5f-4981-9d9c-398f5b2e1a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "\n",
    "data = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c02424a-11b2-4282-903e-a2ec1f0af056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068332</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031988</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "\n",
       "         s4        s5        s6  target  \n",
       "0 -0.002592  0.019907 -0.017646   151.0  \n",
       "1 -0.039493 -0.068332 -0.092204    75.0  \n",
       "2 -0.002592  0.002861 -0.025930   141.0  \n",
       "3  0.034309  0.022688 -0.009362   206.0  \n",
       "4 -0.002592 -0.031988 -0.046641   135.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff = pd.DataFrame(data.data , columns=data.feature_names)\n",
    "dff['target'] = data.target\n",
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a60cd79-45c0-4dea-b1ea-cc907c924012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: [1, 2, 3, 4, 5], &#x27;gamma&#x27;: [&#x27;auto&#x27;, &#x27;scale&#x27;],\n",
       "                         &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;poly&#x27;, &#x27;sigmoid&#x27;, &#x27;rbf&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: [1, 2, 3, 4, 5], &#x27;gamma&#x27;: [&#x27;auto&#x27;, &#x27;scale&#x27;],\n",
       "                         &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;poly&#x27;, &#x27;sigmoid&#x27;, &#x27;rbf&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={'C': [1, 2, 3, 4, 5], 'gamma': ['auto', 'scale'],\n",
       "                         'kernel': ['linear', 'poly', 'sigmoid', 'rbf']})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dff.drop('target' , axis=1)\n",
    "y = dff.target\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X , y , \n",
    "                                                   test_size=0.25,\n",
    "                                                   random_state=42)\n",
    "\n",
    "param = {\n",
    "    \n",
    "    'kernel' : ['linear' , 'poly' , 'sigmoid' , 'rbf'] , \n",
    "    'C' : [1,2,3,4,5] , \n",
    "    'gamma' : ['auto' , 'scale']\n",
    "}\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.svm import SVC\n",
    "svc_model = SVC()\n",
    "\n",
    "clf = GridSearchCV(svc_model , param_grid=param , cv=5)\n",
    "clf.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d32a6c9-18d1-4756-858c-4e665dc9773a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'gamma': 'scale', 'kernel': 'poly'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49f900e6-9b0a-4f7a-9a14-2687912db7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([230., 200., 109., 248., 200., 200., 233., 142., 104., 200., 200.,\n",
       "       178.,  85., 200., 200., 200., 178., 180., 178., 200., 200., 200.,\n",
       "       200., 200., 200., 200., 200., 200.,  59., 200., 200., 200., 200.,\n",
       "       200., 200., 200., 200., 200., 200.,  59., 200., 200., 200.,  89.,\n",
       "       200.,  75.,  55.,  64.,  59., 200., 182., 134., 200., 200., 229.,\n",
       "       200., 200., 200., 200.,  93., 200., 200., 200., 200., 200., 200.,\n",
       "       200., 200., 142., 200., 200., 200., 189., 200., 200., 138., 200.,\n",
       "       200., 200., 200., 200., 200.,  51., 200., 200., 200., 114.,  65.,\n",
       "       200., 200., 200., 200.,  65.,  59.,  65., 200., 275., 200.,  51.,\n",
       "       200., 200.,  55., 258.,  71., 200., 143., 200., 200., 200., 200.,\n",
       "       200.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_clf = clf.best_estimator_\n",
    "\n",
    "model_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f3643a7-cbf1-4096-844b-2190c4a41902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018018018018018018\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model_clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_pred , y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c47ff7-2f49-4941-bf4f-d72a6e374234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
